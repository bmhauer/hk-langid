This software package is made available as part of:
  Bradley Hauer and Grzegorz Kondrak. 2016. Decoding Anagrammed Texts
  Written in Unknown Language and Script. Accepted for publication in
  Transactions of the Association for Computational Linguistics. 

This package contains four programs, implemented as Perl scripts, which 
together serve as the decipherment toolkit presented in the above paper. There
are several additional modules required for these scripts to run. At runtime,
they should be in the same directory as the scripts.

Also, due to limitations of both liscencing and space, we are not able to
include the data we use in our experiments in this package. However, the paper
listed at the beginning of this document describes the data in detail, and
how we pre-process it. For each script, this readme will also explain how the
data should be formatted, how to name your files, etc, as some of this 
information is hard-coded (though, of course, you can always edit the scripts;
see the liscence file for details). This readme will also explain how to run
each of the four scripts, once you have the necessary files.

In general, if you have problems running this code, the best advice is to
read the code and see what is going on; chances are you missed creating
an important file.

Finally, as a reminder (since this was also in the liscence file):
this code is provided without any warranty or guarantees. If your use of
this code somehow damages something, breaks something, or otherwise does 
something you don't like, we are not responsible. By using or modifying this
software, you agree that we are not responsible for what happens as a result.

Now, onto the code.


---
langid_probidst.pl
---

Implements the "Character Frequency" and "Decomposition Pattern Frequency"
language identification methods from the paper. This script is designed
specifically to use the data described in the paper, if you plan to use
different data, make sure it is in exactly the same format, including the file
names.

To run:
perl langid_probdist.pl -c 'CIPHERTEXTS' -p 'SAMPLES' -d METHOD 
where CIPHERTEXTS and SAMPLES are lists of the ciphertexts you want analysed
and the samples of the languages the ciphertexts might be, respectively,
and METHOD is either 'unigram' or 'pattern', depending on which method you 
want to use, character frequency or decomposition pattern frequency.
Wildcards can be used in the ciphertexts and samples.
For example, the command I used to generate my results is:
perl langid_probdist.pl -c 'udhr-unicode/*.tes' -p 'udhr-unicode/*.tra' -d pattern
which uses the pattern method to identify the languages of all .tes files in
the directory udhr-unicode as one of the languages represented by the .tra 
files in the same directory.


---
langid_lmopt.pl
---

Implements the "Character Frequency" and "Decomposition Pattern Frequency"
language identification methods from the paper. This script is designed
specifically to use the data described in the paper, if you plan to use
different data, make sure it is in exactly the same format, including the file
names.

To run:
perl langid_probdist.pl -c 'CIPHERTEXTS' -p 'SAMPLES' -r RESTARTS -f ITERATIONS -b NGRAMS
where CIPHERTEXTS and SAMPLES are lists of the ciphertexts you want analysed
and the samples of the languages the ciphertexts might be, respectively.
Wildcards can be used in the ciphertexts and samples.
RESTARTS is the number of random restarts to use.
ITERATIONS controls the maximum number of greedy search iterations to perform, 
the maximum will be equal to the size of the alphabet times ITERATIONS.
NGRAMS is the number of ngrams to consider in each iteration, see the paper
for more details.
For example, the command I used to generate my results is:
perl langid_lmopt.pl -c 'udhr-unicode/*.tes.0' -p 'udhr-unicode/*.tra' -r 20 -f 5 -b 10
which uses the given parameters to identify the languages of all .tes files in
the directory udhr-unicode as one of the languages represented by the .tra 
files in the same directory.


---
maspa_solver.pl
---

Based on software from the following publication:
Bradley Hauer, Ryan Hayward, Grzegorz Kondrak. 2014. Solving Substitution Ciphers with Combined Language Models. Proceedings of COLING 2014, the 25th International Conference on Computational Linguistics: Technical Papers, pages 2314â€“2325, Dublin, Ireland. Dublin City University and Association for Computational Linguistics.

Get the original software here:
https://github.com/bmhauer/hhk-decipherment

The software is run the same way as the original, except that the training data
must be alphagrams (i.e. sort the letters into each word into alphabetical 
order, so "one two three four" becomes "eno otw eehrt foru"). See the original 
documentation for details.


---
alphagram_solver.pl
---

Decodes anagrammed text (the name comes from the fact that alphagrams, a
special standardized case of anagrams, are used as an internal representation;
see the paper for details). This is essentially a Viterbi decoder with trigram
language models.

Reads input from STDIN, writes output to STDOUT.

Requires Viterbi.pm, ReadLM.pm, Utils.pm, and Alphagram.pm (all included).

To run: cat INPUT | perl alphagram_solver.pl $training_file
where "$training_file" is the text file containing your training corpus

You will also need to create the following files:
$training_file.alp.delint
  - provides the coefficients for the interpolation of the language model
  - here is an entire sample file:
    lambda 3: 444558        0.574687324271393
    lambda 2: 217839        0.281604002249326
    lambda 1: 111168        0.143708673479281
$training_file.alp.unk.lm{1,2,3}
  - contains the probabilities for the {uni,bi,tri}gram language model.
  - file should be of the format 'ngram TAB conditional probability', with
    the words in an ngram separated by tabs
  - here are a few sample lines from a ./lm2 file:
    can     a       0.044715447154
    can     abide   0.008130081301
    can     add     0.008130081301
    can     and     0.004065040650
    can     any     0.012195121951
    can     approach        0.004065040650
$training_file.alp.unk.alp
  - list of all alphagrams which occur in the training data, and the words
    which can generate them, all separated by tabs
  - for example, the line
    eehrt   there   three   ether
    tells the solver that all the words which alphagram to 'eehrt' are 'there',
    'three', and 'ether'
$training_file.alp.1co
  - same format as the above, but contains alphagrams which occur only once in
    the training data, and their solutions, since the solver treats these 
    differently. The idea is that you remove these words from your training
    corpus, replacing them with a '<u>', or "unknown" token, to train the 
    decoder to handle out-of-vocabulary words
    